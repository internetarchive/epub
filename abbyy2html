#!/usr/bin/env python
from gzip import GzipFile
from zipfile import ZipFile
from urllib2 import urlopen, HTTPError

import abbyyhtml

import sys
import getopt
import os
import gzip
import string
import StringIO
import json
import cgi
import re

import iarchive

cache="/tmp/abbyy2html/"

ns = '{http://www.abbyy.com/FineReader_xml/FineReader6-schema-v1.xml}'
page_tag = ns + 'page'
block_tag = ns + 'block'
olibid=False
iabook=False
iaid=False
max_width = 600
max_height = 780
wrapbody=True

def tryenv(var,dflt):
    if (os.environ.get(var)):
        return os.environ.get(var)
    else: return dflt

if (os.path.exists(sys.argv[1])):
    bookid='sbook'
    if sys.argv[1].endswith('.gz'):
        f=gzip.open(sys.argv[1])
    else: f=open(sys.argv[1])
elif (sys.argv[1].startswith('http')):
    bookid='sbook'
    if sys.argv[1].endswith('.gz'):
        f=GzipFile(fileobj=urlopen(sys.argv[1]))
    else: f=urlopen(sys.argv[1])
else:
    if (sys.argv[1].startswith('OL')):
        olibstream=urlopen("http://www.openlibrary.org/books/%s.json"%sys.argv[1])
        olibinfo=json.load(olibstream)
        olibid=sys.argv[1]
        iaid=olibinfo["ocaid"]
        if (not(iaid)):
            error ("No archive reference for %s"%olibid)
    else: iaid=sys.argv[1]
    bookid=sys.argv[1]
    try:
        urlstream=urlopen("http://www.archive.org/download/%s/%s_abbyy.gz"%(iaid,iaid))
        zipdata=urlstream.read()
        f=GzipFile(fileobj=StringIO.StringIO(zipdata))
    except HTTPError:
        urlstream=urlopen("http://www.archive.org/download/%s/%s_abbyy.zip"%(iaid,iaid))
        zipdata=urlstream.read()
        zipfile=ZipFile(StringIO.StringIO(zipdata))
        names=zipfile.namelist()
        f=zipfile.open(names[0])
    detailstream=urlopen("http://www.archive.org/details/%s?output=json"%iaid)
    details=json.load(detailstream)
    if "metadata" in details:
       metadata=details["metadata"]
       if "title" in metadata: title=string.replace(metadata["title"][0],"'","&apos;")
       if "creator" in metadata: creator=string.replace(metadata["creator"][0],"'","&apos;")

if "nowrap" in sys.argv:
    wrapbody=False

pagemerge=(not ("nopagemerge" in sys.argv))
# All the strings to be output (not strictly paragraphs, though)
pars=[]
# The current open paragraph
openpar=False
# The non-body elements processed since the open body paragraph
waiting=[]
# Whether the current paragraph ends with a hyphen
openhyphen=False
classmap={}
if pagemerge:
    for line in abbyyhtml.getblocks(f,bookid,classmap,inline_blocks=True):
        if (len(line)==0):
            pars
        elif (line.startswith("<p")):
            if (openpar):
                firstletter=re.search("(?m)>(\s|'|\")*[a-zA-z]",line)
                firstlower=re.search("(?m)>(\s|'|\")*[a-z]",line)
                if ((not firstletter or not firstlower) or
                    ((firstletter.start()) != (firstlower.start()))):
                    # Not a continuation, so push the open paragraph
                    pars.append(openpar)
                    # add any intervening elements
                    for elt in waiting:
                        pars.append(elt)
                    waiting=[]
                    # and start with a new open paragraph
                    openpar=line
                else:
                    # This paragraph continues the previous one, so
                    #   we append it to openpar, with waiting elements
                    #   added to the middle
                    if openhyphen:
                        par_end=openhyphen.start()
                        openpar=openpar[0:par_end]+"<span class='abbyydroppedhyphen'>-</span>"
                    else:
                        search_end=re.search("(?m)</p>",openpar)
                        if search_end:
                            openpar=openpar[0:(search_end.start())]+" "
                        else: 
                            openpar=openpar+" "
                    for elt in waiting:
                        openpar=openpar+elt
                    waiting=[]
                    textstart=line.find(">")
                    openpar=openpar+line[textstart+1:]
            else:
                # This is the first paragraph
                openpar=line
                for elt in waiting:
                    pars.append(elt)
                waiting=[]
            # Check if the current open paragraph ends with a hyphen
            openhyphen=re.search("(?m)-\s*</p>",openpar)
        else:
            waiting.append(line)
    if openpar:
        pars.append(openpar)
    for elt in waiting:
        pars.append(elt)
else:
    for line in abbyyhtml.getblocks(f,bookid,classmap,inline_blocks=True):
        if (len(line)==0):
            pars
        else:
            pars.append(line)

if wrapbody:
    print "<?xml version='1.0' encoding='utf-8' ?>"
    print "<!DOCTYPE html>"
    print "<html>"
    print "<head>"
    print ("<meta name='ia.item' content='%s'/>"%iaid)
    if olibid:
        print ("<meta name='olib.item' content='%s'/>"%olibid)
    if title and creator:
        print "<title>%s by %s</title>"%(title,creator)
    elif title:
        print "<title>%s</title>"%title
    elif creator:
        print "<title>by %s</title>"%creator
    if title:
        print "<meta name='DC.TITLE' content='%s'/>"%title
    if creator:
        print "<meta name='DC.CREATOR' content='%s'/>"%creator
    if "publisher" in metadata:
        print "<meta name='DC.PUBLISHER' content='%s'/>"%(metadata["publisher"][0])
    if "date" in metadata:
        print "<meta name='DC.DATE' content='%s'/>"%(metadata["date"][0])
    if "description" in metadata:
        print "<meta name='DC.DESCRIPTION' content='%s'/>"%(metadata["description"][0])
    print "<link rel='sbook.refuri' href='http://www.archive.org/sbooks/%s/index.html'/>"%bookid
    print "<style>"
    print ".abbyypagehead { display: none;}"
    print ".abbyypagefoot { display: none;}"
    classhist=classmap['%histogram']
    for x in classmap:
        if not(x.startswith('%')):
            print ".%s { %s } /* %d times */"%(classmap[x],x,classhist[classmap[x]])
    print "</style>"
    print "</head>"
    print "<body>"

for par in pars:
    print par.encode('utf-8')
if wrapbody:
    print "</body>"
    print "</html>"
